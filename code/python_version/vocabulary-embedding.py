
# coding: utf-8

# ## Generate intial word embedding for headlines and description

# - The embedding is limited to a fixed vocabulary size (`vocab_size`) but a vocabulary of all the words that appeared in the data is built.
# 
# - Specify FN0 and FN_lines to correctly use the pickle file in pickles folder. This pickle file should have been generated by load_data.

# In[426]:


FN = 'vocabulary-embedding'
FN0 = 'all-the-news' # this is the name of the data file which I assume you already have
FN_lines = 5000


# In[427]:


seed = 42


# In[428]:


vocab_size = 24483 # vocab size in GloVe


# In[429]:


embedding_dim = 300


# In[430]:


lower = True # whether to lowercase the text


# # Read tokenized headlines and descriptions

# In[431]:


import pickle
with open('pickles/%s_%s.pickle'%(FN0, FN_lines), 'rb') as fp:
    heads, desc, keywords = pickle.load(fp) # keywords are not used in this project


# In[432]:


n_sentences = 5


# In[433]:


if lower:
    heads = [h[0] for h in heads]


# In[434]:


if lower:
    desc = [' '.join(h[:n_sentences]).lower() for h in desc]


# In[435]:


i=0
heads[i]


# In[436]:


desc[i]


# In[437]:


# keywords[i]


# In[438]:


len(heads),len(set(heads))


# In[439]:


len(desc),len(set(desc))


# # Build vocabulary from given pickle file

# In[440]:


from collections import Counter
from itertools import chain
def get_vocab(lst):
    vocabcount = Counter(w for txt in lst for w in txt.split())
    vocab = list(map(lambda x: x[0], sorted(vocabcount.items(), key=lambda x: -x[1])))
    return vocab, vocabcount


# In[441]:


vocab, vocabcount = get_vocab(heads+desc)


# In[442]:


print('Most Popular Tokens:')
print(list(vocab)[:50])
print('...',len(list(vocab)))


# In[443]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
plt.plot([vocabcount[w] for w in vocab]);
plt.gca().set_xscale("log", nonposx='clip')
plt.gca().set_yscale("log", nonposy='clip')
plt.title('word distribution in headlines and description')
plt.xlabel('rank')
plt.ylabel('total appearances')


# always nice to see [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)

# # Index words

# In[444]:


empty = 0 # RNN mask of no data
eos = 1  # end of sentence
start_idx = eos+1 # first real word


# In[445]:


def get_idx(vocab, vocabcount):
    word2idx = dict((word, idx+start_idx) for idx,word in enumerate(vocab))
    word2idx['<empty>'] = empty
    word2idx['<eos>'] = eos
    
    idx2word = dict((idx,word) for word,idx in word2idx.items())

    return word2idx, idx2word


# In[446]:


word2idx, idx2word = get_idx(vocab, vocabcount)


# # Word Embedding

# ## Read GloVe

# In[447]:


from keras.utils.data_utils import get_file
import os

# Since we are using GloVe, we use the text file that matches the dimension specified at the top of this file
# GloVe.6B has dimensions = 50/100/200/300
# fname = 'glove.6B.%dd.txt'%embedding_dim
# fname = 'glove.6B.100d.5k.txt'
fname = 'elmo_10000_dim300.txt'
datadir_base = os.path.expanduser(os.path.join('~', '.keras'))

if not os.access(datadir_base, os.W_OK):
    datadir_base = os.path.join('/tmp', '.keras')
datadir = os.path.join(datadir_base, 'datasets')

glove_name = os.path.join(datadir, fname)
print(glove_name)
if not os.path.exists(glove_name):
    # If the specified text file does not exist, we assume that the glove.6B.zip file has not been downloaded
    path = 'glove.6B.zip'
    path = get_file(path, origin="http://nlp.stanford.edu/data/glove.6B.zip")   
    # Run unzip shell command to extract glove.6B.zip into ~/.keras/datasets
    get_ipython().system('unzip {path} -d {datadir}')


# In[448]:


glove_n_symbols = get_ipython().getoutput('wc -l {glove_name} # Line counts in glove_name (essentially the vocabulary size)')
glove_n_symbols = int(glove_n_symbols[0].split()[0])
glove_n_symbols


# In[449]:


# import numpy as np

glove_index_dict = {} # Keeps track of the index corresponding to each word in the vocabulary
glove_embedding_weights = np.empty((vocab_size, embedding_dim)) # Saves vector representation for each word, ordered by its index
globale_scale = .1

# elmo_set = set()

# with open('/Users/elainelinlinlin/.keras/datasets/elmo_300.txt', 'r') as fp:
#     for l in fp:
#         l = l.strip().split()
#         w = l[0]
#         elmo_set.add(w)

# print(len(elmo_set))
# i = 0
# with open(glove_name, 'r') as fp:
#     with open('/Users/elainelinlinlin/.keras/datasets/glove.6B.100d.5k.txt', 'w') as output:
#         for line in fp:
#             l = line.strip().split()
#             w = l[0]
#             if w in elmo_set:
#                 output.write(line)
#                 i = i + 1

# print(i)


# In[450]:


with open(glove_name, 'r') as fp:
    i = 0
    for l in fp:
        # Each line in glove text file is the following format: { word vector-representation }
        l = l.strip().split() # strip and split each line separated by spaces
        w = l[0] # w is the first column in each line
        glove_index_dict[w] = i
        glove_embedding_weights[i,:] = list(map(float,l[1:]))
        i += 1
glove_embedding_weights *= globale_scale 


# In[451]:


glove_embedding_weights.std()


# In[452]:


for w,i in glove_index_dict.items():
    w = w.lower()
    if w not in glove_index_dict:
        glove_index_dict[w] = i


# ## Embedding Matrix

# Use GloVe to initialize embedding matrix

# In[453]:


# Generate random embedding with same scale as glove
np.random.seed(seed)
shape = (vocab_size, embedding_dim)
# TO-DO: Scale -- why?
scale = glove_embedding_weights.std()*np.sqrt(12)/2 # uniform and not normal
embedding = np.random.uniform(low=-scale, high=scale, size=shape)
print('random-embedding/glove scale', scale, 'std', embedding.std())

# Copy from glove weights of words that appear in our short vocabulary (idx2word)
c = 0
g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))
for i in range(min(vocab_size, len(idx2word))):
    w = idx2word[i] # Word at the i-th index in idx2word
    g = glove_index_dict.get(w, glove_index_dict.get(w.lower())) # The corresponding index of w in glove_index_dict
    if g is not None:
        # Populate the weights for the corresponding words into embedding if the word occurs in glove
        embedding[i,:] = glove_embedding_weights[g,:]
        c += 1
print('number of tokens, in small vocab, found in glove and copied to embedding', c,c/float(vocab_size))


# #### Lots of words in the full vocabulary (word2idx) are not found in GloVe vocabulary:
# Build an alterantive which will map them to their closest match in glove but only if the match
# is good enough (cos distance above `glove_thr`)

# In[454]:


glove_thr = 0.5


# In[455]:


word2glove = {}
for w in word2idx:
    if w in glove_index_dict:
        g = w
    elif w.lower() in glove_index_dict:
        g = w.lower()
    elif w.startswith('#') and w[1:] in glove_index_dict:
        g = w[1:]
    elif w.startswith('#') and w[1:].lower() in glove_index_dict:
        g = w[1:].lower()
    else:
        continue
    word2glove[w] = g


# For every word outside the embedding matrix find the closest word inside the mebedding matrix.
# Use cos distance of GloVe vectors.
# 
# Allow for the last `nb_unknown_words` words inside the embedding matrix to be considered to be outside.
# Dont accept distances below `glove_thr`

# In[456]:


normed_embedding = embedding/np.array([np.sqrt(np.dot(gweight,gweight)) for gweight in embedding])[:,None]

nb_unknown_words = 100

glove_match = []
for w,idx in word2idx.items():
    if idx >= vocab_size-nb_unknown_words and w.isalpha() and w in word2glove:
        gidx = glove_index_dict[word2glove[w]]
        gweight = glove_embedding_weights[gidx,:].copy()
        # find row in embedding that has the highest cos score with gweight
        gweight /= np.sqrt(np.dot(gweight,gweight))
        score = np.dot(normed_embedding[:vocab_size-nb_unknown_words], gweight)
        while True:
            embedding_idx = score.argmax()
            s = score[embedding_idx]
            if s < glove_thr:
                break
            if idx2word[embedding_idx] in word2glove :
                glove_match.append((w, embedding_idx, s)) 
                break
            score[embedding_idx] = -1
glove_match.sort(key = lambda x: -x[2])
print('# of glove substitutes found', len(glove_match))


# manually check that the worst substitutions we are going to do are good enough

# In[457]:


for orig, sub, score in glove_match[-10:]:
    print(score, orig,'=>', idx2word[sub])


# Build a lookup table of index of outside words to index of inside words

# In[458]:


glove_idx2idx = dict((word2idx[w],embedding_idx) for  w, embedding_idx, _ in glove_match)


# # Data

# In[459]:


# Each row in Y is an array that represents a headline word by word, denoted by its index mapped by word2idx.
# e.g. 
#  heads[0].split() == ['New', 'York', 'Time']
#  Y[0] == [1065, 1424, 5309]
#  word2idx['New'] == 1065
# len(Y) should match len(heads)
Y = [[word2idx[token] for token in headline.split()] for headline in heads]
len(Y)


# In[460]:


plt.hist(list(map(len,Y)),bins=50)
plt.title('Length of headlines distribution')
plt.xlabel('Length of headlines')
plt.ylabel('Total occurrences')


# In[461]:


# Each row in X is an array that represents a description word by word, denoted by its index mapped by word2idx.
# e.g. 
#  desc[0].split() == ['New', 'York', 'Time']
#  X[0] == [1065, 1424, 5309]
#  word2idx['New'] == 1065
# len(X) should match len(desc)
X = [[word2idx[token] for token in d.split()] for d in desc]
len(X)


# In[462]:


plt.hist(list(map(len,X)),bins=50);
plt.title('Length of description distribution')
plt.xlabel('Length of descriptions')
plt.ylabel('Total occurrences')


# In[463]:


with open('data/%s-%s-vocab-embedding-elmo-10000.pickle'%(FN0, FN_lines),'wb') as fp:
    pickle.dump((embedding, idx2word, word2idx, glove_idx2idx),fp,-1)


# In[464]:


with open('data/%s-%s-vocab-embedding-elmo-10000.data.pickle'%(FN0, FN_lines),'wb') as fp:
    pickle.dump((X,Y),fp,-1)


# In[465]:


print('Vocabulary embedding completed!')

